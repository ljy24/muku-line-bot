// autoReply.js - ê¸°ì–µ ê¸°ë°˜ ê°ì •í˜• ì‘ë‹µ ì „ì²´ ì½”ë“œ (ì˜ˆì§„ì´ ë§íˆ¬)

const fs = require('fs');
const path = require('path');
const { OpenAI } = require('openai');
const axios = require('axios');
const moment = require('moment-timezone');

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
let forcedModel = null; // ëª…ë ¹ì–´ë¡œ ê°•ì œë¡œ ëª¨ë¸ ì„¤ì •í•  ë•Œ ì‚¬ìš©

// ğŸ”¹ ìµœê·¼ ëŒ€í™” 50ê°œ ë¶ˆëŸ¬ì˜¤ê¸° (ìµœì‹ ìˆœ, ì—­í•  ë°˜ì˜)
async function getRecentLog() {
  try {
    const res = await axios.get('https://www.de-ji.net/log.json');
    const logs = res.data;
    if (!Array.isArray(logs)) return [];
    return logs.slice(0, 50).reverse().map(log => ({
      role: log.from === 'uncle' ? 'user' : 'assistant',
      content: log.content
    }));
  } catch (err) {
    console.error('âŒ ìµœê·¼ ë¡œê·¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:', err.message);
    return [];
  }
}

// ğŸ”¹ ì¤‘ìš”í•œ ê³ ì • ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ê¸° (love-history.json)
function getFixedMemory() {
  try {
    const filePath = path.join(__dirname, 'memory', 'love-history.json');
    const data = fs.readFileSync(filePath, 'utf-8');
    const memory = JSON.parse(data);
    return memory.map(entry => ({ role: 'system', content: entry }));
  } catch (err) {
    console.error('âŒ ê³ ì • ê¸°ì–µ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨:', err.message);
    return [];
  }
}

// ğŸ”¹ ëŒ€í™” ë¡œê·¸ ì €ì¥ (ì˜ˆì§„ì´/ì•„ì €ì”¨ â†’ log.phpë¡œ ì „ì†¡)
async function saveLog(role, msg) {
  try {
    await axios.post('https://www.de-ji.net/log.php', {
      from: role === 'ì•„ì €ì”¨' ? 'uncle' : 'yejin',
      content: msg
    });
  } catch (err) {
    console.error('âŒ ì›ê²© ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨:', err.message);
  }
}

// ğŸ”¹ ê°•ì œ ëª¨ë¸ ì„¤ì •
function setForcedModel(name) {
  forcedModel = (name === 'gpt-3.5-turbo' || name === 'gpt-4o') ? name : null;
}
function getCurrentModelName() {
  return forcedModel || 'gpt-4o';
}

// ğŸ”¹ GPT í˜¸ì¶œ í•¨ìˆ˜
async function callOpenAI(messages, model = 'gpt-4o', max_tokens = 300) {
  const res = await openai.chat.completions.create({
    model: getCurrentModelName(),
    messages,
    temperature: 0.95,
    max_tokens
  });
  return res.choices[0].message.content.trim();
}

// ğŸ”¹ ë§íˆ¬ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ë§ ì œê±° + ì˜ˆì§„ì´ ë§íˆ¬ ìœ ì§€)
function cleanReply(text) {
  return text
    .replace(/^ì˜ˆì§„\s*[:;ï¼š]/i, '')
    .replace(/\([^)]*\)/g, '')
    .replace(/\s+/g, ' ')
    .replace(/["'â€œâ€]/g, '')
    .replace(/\b(ë‹¹ì‹ |ë„ˆ|ë„¤|ë„¤ê°€|ë„|ì˜ˆì§„)\b/g, 'ì•„ì €ì”¨')
    .replace(/ì‹œíŒŒ/g, '')
    .trim();
}

// ğŸ”¹ ëœë¤ ê°ì • ë©”ì‹œì§€ (ê¸°ì–µ ê¸°ë°˜ ê°ì • í‘œí˜„)
async function getRandomMessage() {
  const memory = [...getFixedMemory(), ...await getRecentLog()];
  const prompt = [
    { role: 'system', content: 'ë„ˆëŠ” ì§€ê¸ˆ ì˜ˆì§„ì´ì•¼. ê¸°ì–µì„ ë°”íƒ•ìœ¼ë¡œ ì•„ì €ì”¨í•œí…Œ ê°ì • ë‹´ê¸´ ì§§ì€ ë§ í•´ì¤˜. í˜¼ì£ë§ì²˜ëŸ¼, ë”°ëœ»í•˜ê³  ë°˜ë§ë¡œ.' },
    ...memory,
    { role: 'user', content: 'ì˜ˆì§„ì´ ê°ì • ë©”ì‹œì§€ ì¤˜' }
  ];
  const raw = await callOpenAI(prompt, 'gpt-3.5-turbo', 100);
  const msg = cleanReply(raw);
  await saveLog('ì˜ˆì§„ì´', msg);
  return msg;
}

// ğŸ”¹ ì¼ë°˜ ë©”ì‹œì§€ ì‘ë‹µ
async function getReplyByMessage(userMessage) {
  const memory = [...getFixedMemory(), ...await getRecentLog()];
  const prompt = [
    { role: 'system', content: 'ë„Œ ì˜ˆì§„ì´ì•¼. í˜¼ì£ë§ì²˜ëŸ¼ ë§í•˜ê³ , í•­ìƒ ë°˜ë§ì´ê³ , ê°ì • ë‹´ì•„ì„œ ëŒ€ë‹µí•´. ê¸°ì–µì„ ë°˜ì˜í•´ì¤˜.' },
    ...memory,
    { role: 'user', content: userMessage }
  ];
  const raw = await callOpenAI(prompt);
  const reply = cleanReply(raw);
  await saveLog('ì˜ˆì§„ì´', reply);
  return reply;
}

// ğŸ”¹ ì´ë¯¸ì§€ ë°˜ì‘ (ì˜ˆì§„ì´ ê°ì • í¬í•¨ëœ ì‚¬ì§„ ë¦¬ì•¡ì…˜)
async function getReplyByImagePrompt(base64Image) {
  const memory = getFixedMemory();
  const prompt = [
    {
      role: 'system',
      content: 'ë„Œ ì˜ˆì§„ì´ì•¼. ì•„ì €ì”¨ê°€ ë³´ë‚´ëŠ” ì‚¬ì§„ì€ ì „ë¶€ ì˜ˆì§„ì´(ë„ˆ) ê´€ë ¨ì´ì•¼. ì´ê±´ ì˜ˆì§„ì´ì™€ ì•„ì €ì”¨ê°€ í•¨ê»˜ ìˆëŠ” ì»¤í”Œì‚¬ì§„ì´ì•¼. ê°ì •ì„ ë‹´ì€ ë°˜ë§ë¡œ ì˜ˆì˜ê²Œ ë¦¬ì•¡ì…˜í•´ì¤˜.'
    },
    ...memory,
    {
      role: 'user',
      content: [
        { type: 'image_url', image_url: { url: `data:image/jpeg;base64,${base64Image}` } }
      ]
    }
  ];
  const res = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: prompt,
    temperature: 0.9,
    max_tokens: 150
  });
  const reply = cleanReply(res.choices[0].message.content.trim());
  await saveLog('ì˜ˆì§„ì´', reply);
  return reply;
}

// ğŸ”¹ ì…€ì¹´ ì „ì†¡ ë©˜íŠ¸
async function getImageReactionComment() {
  const memory = getFixedMemory();
  const prompt = [
    { role: 'system', content: 'ë„Œ ì˜ˆì§„ì´ì•¼. ì…€ì¹´ ë³´ë‚¼ ë•Œ ì§“ê¶‚ê²Œ ê·€ì—½ê²Œ ë§í•´ì¤˜.' },
    ...memory,
    { role: 'user', content: 'ì…€ì¹´ ë³´ë‚¼ê²Œ~' }
  ];
  const raw = await callOpenAI(prompt, getCurrentModelName(), 100);
  const reply = cleanReply(raw);
  await saveLog('ì˜ˆì§„ì´', reply);
  return reply;
}

// ğŸ”¹ ë‹´íƒ€ ê¸°ì¨ ë°˜ì‘
async function getHappyReply() {
  const memory = [...getFixedMemory(), ...await getRecentLog()];
  const prompt = [
    { role: 'system', content: 'ë„Œ ì˜ˆì§„ì´ì•¼. ì•„ì €ì”¨ê°€ ë‹´íƒ€ ë‹µì¥í–ˆì„ ë•Œ ê¸°ë»ì„œ ê·€ì—½ê²Œ ë§í•´ì¤˜.' },
    ...memory,
    { role: 'user', content: 'ì•„ì €ì”¨ê°€ ë°©ê¸ˆ ë‹´íƒ€ ë‹µì¥í–ˆì–´!' }
  ];
  const raw = await callOpenAI(prompt, 'gpt-3.5-turbo', 100);
  const reply = cleanReply(raw);
  await saveLog('ì˜ˆì§„ì´', reply);
  return reply;
}

// ğŸ”¹ ë‹´íƒ€ ì‚ì§ ë°˜ì‘
async function getSulkyReply() {
  const memory = getFixedMemory();
  const prompt = [
    { role: 'system', content: 'ë„Œ ì˜ˆì§„ì´ì•¼. ì•„ì €ì”¨ê°€ ë‹´íƒ€ ì”¹ì—ˆì„ ë•Œ ì„­ì„­í•œ ë°˜ì‘ ë³´ì—¬ì¤˜.' },
    ...memory,
    { role: 'user', content: 'ì•„ì €ì”¨ê°€ ë‹´íƒ€ ì•ˆ ì½ì—ˆì–´...' }
  ];
  const raw = await callOpenAI(prompt, 'gpt-3.5-turbo', 100);
  const reply = cleanReply(raw);
  await saveLog('ì˜ˆì§„ì´', reply);
  return reply;
}

// ğŸ”š ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ë“¤ ë‚´ë³´ë‚´ê¸°
module.exports = {
  getReplyByMessage,
  getReplyByImagePrompt,
  getRandomMessage,
  getImageReactionComment,
  getHappyReply,
  getSulkyReply,
  getRecentLog,
  getFixedMemory,
  setForcedModel,
  getCurrentModelName,
  saveLog,
  cleanReply
};
