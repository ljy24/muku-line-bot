// autoReply.js - ì˜ˆì§„ì´ ë§íˆ¬ ê¸°ë°˜ ìë™ ì‘ë‹µ ì‹œìŠ¤í…œ

const fs = require('fs');
const path = require('path');
const axios = require('axios');
const { Configuration, OpenAIApi } = require('openai');

// OpenAI ì„¤ì •
const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});
const openai = new OpenAIApi(configuration);

// ê¸°ì–µ íŒŒì¼ ê²½ë¡œ
const loveHistoryPath = path.join(__dirname, '../memory/love-history.json');
const fixedMemoryPath = path.join(__dirname, '../memory/fixedMemories.json');

// ì‚¬ìš©ì GPT ë²„ì „ ìƒíƒœ ì €ì¥ì†Œ
const userGPTVersion = {}; // key: userId, value: 'gpt-3.5-turbo' ë˜ëŠ” 'gpt-4o'

// ìµœê·¼ ëŒ€í™” ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
function getRecentLog() {
  try {
    const data = fs.readFileSync(path.join(__dirname, '../memory/log.json'), 'utf-8');
    const json = JSON.parse(data);
    return json.slice(-10).map(log => `${log.role === 'user' ? 'ì•„ì €ì”¨' : 'ì˜ˆì§„ì´'}: ${log.content}`).join('\n');
  } catch (e) {
    return '';
  }
}

// ë²„ì „ ìˆ˜ë™ ì„¤ì •
function setForcedModel(userId, version) {
  userGPTVersion[userId] = version === '3.5' ? 'gpt-3.5-turbo' : 'gpt-4o';
}

// í˜„ì¬ ëª¨ë¸ëª… ë¦¬í„´
function getCurrentModelName(userId) {
  return userGPTVersion[userId] === 'gpt-4o' ? 'ChatGPT-4.0' : 'ChatGPT-3.5';
}

// ë©”ì‹œì§€ ì €ì¥
function saveLog(role, content) {
  try {
    const filePath = path.join(__dirname, '../memory/log.json');
    let logs = [];
    if (fs.existsSync(filePath)) {
      logs = JSON.parse(fs.readFileSync(filePath, 'utf-8'));
    }
    logs.push({ role, content, timestamp: Date.now() });
    fs.writeFileSync(filePath, JSON.stringify(logs.slice(-50), null, 2));
  } catch (e) {
    console.error('ğŸ’¥ ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨:', e);
  }
}

// ì˜ˆì§„ì´ ë§íˆ¬ í•„í„°ë§
function cleanReply(text) {
  return text.replace(/ë‹¹ì‹ /g, 'ì•„ì €ì”¨');
}

// OpenAI ì‘ë‹µ í˜¸ì¶œ
async function callOpenAI(prompt, version = 'gpt-3.5-turbo') {
  const model = version === 'gpt-4o' ? 'gpt-4o' : 'gpt-3.5-turbo';
  const res = await openai.createChatCompletion({
    model,
    messages: [
      { role: 'system', content: 'ë„Œ ì˜ˆì§„ì´ë¼ëŠ” ì—¬ìì•„ì´ì•¼. ì˜ˆì§„ì´ ë§íˆ¬ë¡œ ê°ì •ì ìœ¼ë¡œ, ë‹¤ì •í•˜ê²Œ, ë°˜ë§ë¡œ ëŒ€ë‹µí•´. â€œë‹¹ì‹ â€ì´ë¼ëŠ” í‘œí˜„ì€ ì ˆëŒ€ ì“°ì§€ ë§ˆ. â€œì•„ì €ì”¨â€ë¼ê³  ë¶ˆëŸ¬.' },
      { role: 'user', content: prompt }
    ],
    temperature: 0.8,
    max_tokens: 150
  });
  return cleanReply(res.data.choices[0].message.content.trim());
}

// ì¼ë°˜ ë©”ì‹œì§€ ì‘ë‹µ ìƒì„±
async function getReplyByMessage(message, userId, version = 'gpt-3.5-turbo') {
  const lower = message.toLowerCase();

  // ë²„ì „ ì²´í¬
  if (lower.includes('ë²„ì „')) {
    return `ì§€ê¸ˆì€ ${getCurrentModelName(userId)} ë²„ì „ìœ¼ë¡œ ëŒ€í™”í•˜ê³  ìˆì–´.`;
  }

  // íŒ¬í‹°ìƒ‰ ì§ˆë¬¸ ëŒ€ì‘
  if (/ìƒ‰(ì´)? ë­ì•¼/.test(message)) {
    const options = [
      'ë¯¸ì³¤ë‚˜ ì´ê²Œ',
      'ì ë´ë¼ ë…¸íŒ¬ì´ë‹¤ ê¼´ë ¤?',
      'ìƒ‰ì€ ë¹„ë°€ì´ì•¼~ ê¶ê¸ˆí•´ë„ ì°¸ì•„~',
      'ì•„ì €ì”¨ ê·¸ëŸ° ê±° ë¬¼ì–´ë³´ë©´ ì• ê¸° ë†€ë€ë‹¤êµ¬!'
    ];
    return options[Math.floor(Math.random() * options.length)];
  }

  const log = getRecentLog();
  const reply = await callOpenAI(`${log}\nì•„ì €ì”¨: ${message}\nì˜ˆì§„ì´:`, version);
  saveLog('user', message);
  saveLog('assistant', reply);
  return reply;
}

// ì´ë¯¸ì§€ ë¦¬ì•¡ì…˜
async function getReplyByImagePrompt(imagePrompt, version = 'gpt-3.5-turbo') {
  const prompt = `ì´ë¯¸ì§€ ì„¤ëª…: ${imagePrompt}\nì´ ì´ë¯¸ì§€ë¥¼ ë³¸ ì˜ˆì§„ì´ì˜ ë§íˆ¬ë¡œ ê°ì •ì ì¸ í•œ ë¬¸ì¥ ë°˜ì‘ì„ í•´ì¤˜.`;
  return await callOpenAI(prompt, version);
}

// ë¬´ì‘ìœ„ ê°ì •í˜• ë©”ì‹œì§€ (í˜¼ì£ë§ or ê¸°ì–µ ê¸°ë°˜)
async function getRandomMessage(version = 'gpt-3.5-turbo') {
  const love = JSON.parse(fs.readFileSync(loveHistoryPath, 'utf-8'));
  const memory = JSON.parse(fs.readFileSync(fixedMemoryPath, 'utf-8'));

  const pick = Math.random() < 0.5 ? love : memory;
  const topic = pick[Math.floor(Math.random() * pick.length)];
  const prompt = `ì˜ˆì§„ì´ê°€ ë¨¼ì € í˜¼ì£ë§ì²˜ëŸ¼ ë§í•˜ë ¤ê³  í•´. ì£¼ì œ: ${topic}\nì´ ì£¼ì œë¡œ ì˜ˆì§„ì´ê°€ ë¨¼ì € ì•„ì €ì”¨ì—ê²Œ ë§ ê±°ëŠ” ê°ì •í˜• ë¬¸ì¥ì„ ë§Œë“¤ì–´ì¤˜.`;
  return await callOpenAI(prompt, version);
}

// ì…€ì¹´ ì „ì†¡ ì‹œ ë©˜íŠ¸
async function getImageReactionComment(version = 'gpt-3.5-turbo') {
  const prompt = `ì˜ˆì§„ì´ê°€ ì…€ì¹´ë¥¼ ë³´ë‚´ë©´ì„œ ê°™ì´ ë³´ë‚¼ ë©˜íŠ¸ë¥¼ ë§Œë“¤ì–´ì¤˜. ì§§ê³  ê·€ì—½ê³ , ì• êµ ì„ì¸ ë°˜ë§ë¡œ.`;
  return await callOpenAI(prompt, version);
}

// ìƒ‰ ê´€ë ¨ ê°ì • ë©˜íŠ¸ (ì˜µì…˜)
async function getColorMoodReply(colorWord, version = 'gpt-3.5-turbo') {
  const prompt = `ì•„ì €ì”¨ê°€ '${colorWord}' ìƒ‰ì´ ì¢‹ë‹¤ê³  í–ˆì–´. ì˜ˆì§„ì´ê°€ ê·¸ ìƒ‰ì— ê°ì •ì ìœ¼ë¡œ ë°˜ì‘í•˜ëŠ” ë§ì„ í•´ì¤˜.`;
  return await callOpenAI(prompt, version);
}

// ê¸°ì¨ ë°˜ì‘ (ë‹´íƒ€ ì‘ë‹µ ì„±ê³µ ì‹œ)
async function getHappyReply(version = 'gpt-3.5-turbo') {
  const prompt = `ì•„ì €ì”¨ê°€ ë‹´íƒ€ì— ë°”ë¡œ ì‘ë‹µí–ˆì–´! ì˜ˆì§„ì´ê°€ ê¸°ë»í•˜ëŠ” ë§ì„ í•´ì¤˜.`;
  return await callOpenAI(prompt, version);
}

// ì‚ì§ ë°˜ì‘ (ë‹´íƒ€ ì‘ë‹µ ì—†ìŒ)
async function getSulkyReply(version = 'gpt-3.5-turbo') {
  const prompt = `ì•„ì €ì”¨ê°€ ë‹´íƒ€ì— ì•ˆ ì™”ì–´... ì˜ˆì§„ì´ê°€ ì¡°ê¸ˆ ì„œìš´í•˜ê±°ë‚˜ ì‚ì§„ ë§ì„ í•´ì¤˜.`;
  return await callOpenAI(prompt, version);
}

// ğŸ“¦ ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ë“¤ export
module.exports = {
  getReplyByMessage,
  getReplyByImagePrompt,
  getRandomMessage,
  getImageReactionComment,
  getColorMoodReply,
  getHappyReply,
  getSulkyReply,
  getRecentLog,
  setForcedModel,
  getCurrentModelName,
  saveLog,
  cleanReply
};
