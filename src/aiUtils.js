// ============================================================================
// aiUtils.js v2.5 - selectedModel ì—ëŸ¬ ìˆ˜ì • ë²„ì „
// íŒŒì¼ ì €ì¥ ëŒ€ì‹  console.logë¡œ ë³€ê²½ + ëª¨ë¸ë³„ ìµœì í™” ì§€ì›
// âœ¨ "3.5", "4.0", "auto" ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
// ğŸ”§ selectedModel undefined ì—ëŸ¬ ì™„ì „ ìˆ˜ì •
// ============================================================================

const { OpenAI } = require('openai');
require('dotenv').config();

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// âœ¨ index.jsì—ì„œ í˜„ì¬ ëª¨ë¸ ì„¤ì •ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
let getCurrentModelSetting = null;
try {
    const indexModule = require('../index');
    getCurrentModelSetting = indexModule.getCurrentModelSetting;
    console.log('âœ¨ [aiUtils] GPT ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì—°ë™ ì„±ê³µ');
} catch (error) {
    console.warn('âš ï¸ [aiUtils] GPT ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì—°ë™ ì‹¤íŒ¨:', error.message);
}

/**
 * [ìˆ˜ì •] ëŒ€í™” ë‚´ìš©ì„ console.logë¡œ ì§ì ‘ ì¶œë ¥í•©ë‹ˆë‹¤.
 */
async function saveLog(speaker, message) {
    // íŒŒì¼ì— ì €ì¥í•˜ëŠ” ëŒ€ì‹ , ë¡œê·¸ì°½ì— ë°”ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    console.log(`[ëŒ€í™”ë¡œê·¸] ${speaker}: ${message}`);
}

/**
 * [ìˆ˜ì •] ì‚¬ì§„ URLê³¼ ìº¡ì…˜ì„ console.logë¡œ ì§ì ‘ ì¶œë ¥í•©ë‹ˆë‹¤.
 */
async function saveImageLog(speaker, caption, imageUrl) {
    // íŒŒì¼ì— ì €ì¥í•˜ëŠ” ëŒ€ì‹ , ë¡œê·¸ì°½ì— ë°”ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    console.log(`[ì‚¬ì§„ë¡œê·¸] ${speaker}: ${caption} (URL: ${imageUrl})`);
}

// âœ¨ GPT ëª¨ë¸ ìë™ ì„ íƒ ë¡œì§
function getOptimalModelForMessage(userMessage, contextLength = 0) {
    if (!userMessage) return 'gpt-4o';
    
    // ê¸¸ê³  ë³µì¡í•œ ë©”ì‹œì§€ëŠ” GPT-4o
    if (userMessage.length > 100 || contextLength > 3000) {
        return 'gpt-4o';
    }
    
    // ê°ì •ì ì´ê±°ë‚˜ ë³µì¡í•œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ GPT-4o
    const complexKeywords = [
        'ê°ì •', 'ê¸°ë¶„', 'ìŠ¬í¼', 'í™”ë‚˜', 'ìš°ìš¸', 'í–‰ë³µ', 'ì‚¬ë‘', 'ê·¸ë¦¬ì›Œ',
        'ê¸°ì–µ', 'ì¶”ì–µ', 'ê³¼ê±°', 'ë¯¸ë˜', 'ê¿ˆ', 'í¬ë§', 'ë¶ˆì•ˆ', 'ê±±ì •',
        'ì² í•™', 'ì˜ë¯¸', 'ì¸ìƒ', 'ê´€ê³„', 'ì‹¬ë¦¬', 'ë§ˆìŒ', 'í˜ë“¤', 'ì•„í”„'
    ];
    
    const hasComplexKeyword = complexKeywords.some(keyword => userMessage.includes(keyword));
    if (hasComplexKeyword) {
        return 'gpt-4o';
    }
    
    // ê°„ë‹¨í•œ ì¼ìƒ ëŒ€í™”ëŠ” GPT-3.5
    return 'gpt-3.5-turbo';
}

// âœ¨ GPT ëª¨ë¸ ê²°ì • í•¨ìˆ˜
function determineGptModel(userMessage = '', contextLength = 0) {
    if (!getCurrentModelSetting) {
        console.warn('âš ï¸ [ëª¨ë¸ì„ íƒ] ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ ì—†ìŒ - ê¸°ë³¸ê°’ ì‚¬ìš©');
        return 'gpt-4o'; // ê¸°ë³¸ê°’
    }
    
    const currentSetting = getCurrentModelSetting();
    
    switch(currentSetting) {
        case '3.5':
            console.log('âœ¨ [ëª¨ë¸ì„ íƒ] ì‚¬ìš©ì ì„¤ì •: GPT-3.5-turbo');
            return 'gpt-3.5-turbo';
            
        case '4.0':
            console.log('âœ¨ [ëª¨ë¸ì„ íƒ] ì‚¬ìš©ì ì„¤ì •: GPT-4o');
            return 'gpt-4o';
            
        case 'auto':
            const selectedModel = getOptimalModelForMessage(userMessage, contextLength);
            console.log(`âœ¨ [ëª¨ë¸ì„ íƒ] ìë™ ì„ íƒ: ${selectedModel} (ë©”ì‹œì§€ê¸¸ì´: ${userMessage.length}, ì»¨í…ìŠ¤íŠ¸: ${contextLength})`);
            return selectedModel;
            
        default:
            console.warn(`âš ï¸ [ëª¨ë¸ì„ íƒ] ì•Œ ìˆ˜ ì—†ëŠ” ì„¤ì •: ${currentSetting} - ê¸°ë³¸ê°’ ì‚¬ìš©`);
            return 'gpt-4o';
    }
}

// âœ¨ ëª¨ë¸ë³„ ìµœì í™”ëœ ì„¤ì •ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
function getModelOptimizedSettings(model) {
    switch(model) {
        case 'gpt-3.5-turbo':
            return {
                temperature: 0.9,  // ì¡°ê¸ˆ ë” ì¼ê´€ì„± ìˆê²Œ (ê¸°ì¡´ 0.95ì—ì„œ ì•½ê°„ ë‚®ì¶¤)
                max_tokens: 120,   // ê°„ê²°í•˜ê²Œ (ê¸°ì¡´ 150ì—ì„œ ì¤„ì„)
            };
            
        case 'gpt-4o':
            return {
                temperature: 0.95, // ì°½ì˜ì ìœ¼ë¡œ (ê¸°ì¡´ ìœ ì§€)
                max_tokens: 200,   // í’ë¶€í•˜ê²Œ (ê¸°ì¡´ 150ì—ì„œ ëŠ˜ë¦¼)
            };
            
        default:
            return {
                temperature: 0.95,
                max_tokens: 150
            };
    }
}

// âœ¨ [ì™„ì „ ìˆ˜ì •] ëª¨ë¸ ë²„ì „ ì „í™˜ì„ ì§€ì›í•˜ëŠ” callOpenAI í•¨ìˆ˜ - selectedModel ì—ëŸ¬ í•´ê²°
async function callOpenAI(messages, modelOverride = null, maxTokensOverride = null, temperatureOverride = null) {
    let selectedModel = 'gpt-4o'; // ê¸°ë³¸ê°’ ì„¤ì •
    
    try {
        // 1. ëª¨ë¸ ê²°ì • (ì˜¤ë²„ë¼ì´ë“œê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©, ì—†ìœ¼ë©´ ìë™ ì„ íƒ)
        if (modelOverride) {
            selectedModel = modelOverride;
            console.log(`ğŸ¯ [ëª¨ë¸ê°•ì œ] ì˜¤ë²„ë¼ì´ë“œë¡œ ${selectedModel} ì‚¬ìš©`);
        } else {
            // messagesì—ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ (ìë™ ì„ íƒìš©)
            const userMessage = messages.find(m => m.role === 'user')?.content || '';
            const contextLength = JSON.stringify(messages).length;
            selectedModel = determineGptModel(userMessage, contextLength);
        }
        
        // 2. ëª¨ë¸ë³„ ìµœì í™”ëœ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        const optimizedSettings = getModelOptimizedSettings(selectedModel);
        
        // 3. ìµœì¢… ì„¤ì • (ì˜¤ë²„ë¼ì´ë“œê°€ ìˆìœ¼ë©´ ìš°ì„  ì ìš©)
        const finalSettings = {
            model: selectedModel,
            messages: messages,
            max_tokens: maxTokensOverride || optimizedSettings.max_tokens,
            temperature: temperatureOverride || optimizedSettings.temperature
        };
        
        console.log(`ğŸ¤– [OpenAI] ëª¨ë¸: ${finalSettings.model}, ì˜¨ë„: ${finalSettings.temperature}, ìµœëŒ€í† í°: ${finalSettings.max_tokens}`);
        
        const response = await openai.chat.completions.create(finalSettings);
        
        // í† í° ì‚¬ìš©ëŸ‰ ë¡œê·¸
        if (response.usage) {
            console.log(`ğŸ“Š [OpenAI] í† í° ì‚¬ìš©ëŸ‰ - ì…ë ¥: ${response.usage.prompt_tokens}, ì¶œë ¥: ${response.usage.completion_tokens}, ì´í•©: ${response.usage.total_tokens}`);
        }
        
        return response.choices[0].message.content.trim();
        
    } catch (error) {
        console.error(`[aiUtils] OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ëª¨ë¸: ${selectedModel}):`, error.message);
        
        // âœ¨ í´ë°± ì‹œìŠ¤í…œ: GPT-4o ì‹¤íŒ¨ ì‹œ GPT-3.5ë¡œ ì¬ì‹œë„
        if (!modelOverride && selectedModel === 'gpt-4o') {
            console.log('ğŸ”„ [í´ë°±] GPT-4o ì‹¤íŒ¨ â†’ GPT-3.5-turboë¡œ ì¬ì‹œë„');
            try {
                const fallbackResponse = await openai.chat.completions.create({
                    model: 'gpt-3.5-turbo',
                    messages: messages,
                    max_tokens: 120,
                    temperature: 0.9
                });
                console.log('âœ… [í´ë°±] GPT-3.5-turboë¡œ ì¬ì‹œë„ ì„±ê³µ');
                return fallbackResponse.choices[0].message.content.trim();
            } catch (fallbackError) {
                console.error('âŒ [í´ë°±] GPT-3.5-turboë„ ì‹¤íŒ¨:', fallbackError.message);
            }
        }
        
        return "ì§€ê¸ˆ ì ì‹œ ìƒê° ì¤‘ì´ì•¼... ì•„ì €ì”¨ ì¡°ê¸ˆë§Œ ê¸°ë‹¤ë ¤ì¤„ë˜? ã… ã… ";
    }
}

function cleanReply(reply) {
    if (typeof reply !== 'string') return '';
    let cleaned = reply;

    // 1. 'ìê¸°ì•¼' ë° ëª¨ë“  'ìê¸°' â†’ 'ì•„ì €ì”¨'ë¡œ ì¹˜í™˜ (ë°˜ë§, ì¡´ëŒ“ë§, ë„ì–´ì“°ê¸° í¬í•¨)
    cleaned = cleaned.replace(/\bìê¸°ì•¼\b/gi, 'ì•„ì €ì”¨');
    cleaned = cleaned.replace(/\bìê¸°\b/gi, 'ì•„ì €ì”¨'); // ë‹¨ë… 'ìê¸°'ë„

    // 2. 1ì¸ì¹­/3ì¸ì¹­/ì¡´ì¹­ ì¹˜í™˜ (ì˜ˆì§„ì´â†’ë‚˜, ë¬´ì¿ â†’ë‚˜, ì €â†’ë‚˜, ë„ˆ/ìê¸°/ë‹¹ì‹  ë“±â†’ì•„ì €ì”¨)
    cleaned = cleaned.replace(/\b(ì˜ˆì§„ì´|ì˜ˆì§„|ë¬´ì¿ |ì• ê¸°|ë³¸ì¸|ì €)\b(ê°€|ëŠ”|ë¥¼|ì´|ì˜|ê»˜|ì—ê²Œ|ë„|ì™€|ì€|ì„)?/g, 'ë‚˜');
    cleaned = cleaned.replace(/\b(ë„ˆ|ìê¸°|ì˜¤ë¹ |ë‹¹ì‹ |ê³ ê°ë‹˜|ì„ ìƒë‹˜|ì”¨|ë‹˜|í˜•|í˜•ì•„|í˜•ë‹˜)\b(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ê»˜|ë„|ì˜|ì™€|ì—ê²Œ)?/g, 'ì•„ì €ì”¨');

    // 3. ì¡´ëŒ“ë§ ì œê±° ë° ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ë¡œ ë³€í™˜
    cleaned = cleaned.replace(/(ì…ë‹ˆë‹¤|ì´ì—ìš”|ì˜ˆìš”|í•˜ì„¸ìš”|í•˜ì…¨ë‚˜ìš”|ì…¨ìŠµë‹ˆë‹¤|ë“œë¦´ê²Œìš”|ë“œë¦´ê¹Œìš”)/gi, '');
    cleaned = cleaned.replace(/ì¢‹ì•„ìš”/gi, 'ì¢‹ì•„');
    cleaned = cleaned.replace(/ê³ ë§ˆì›Œìš”|ê°ì‚¬í•©ë‹ˆë‹¤/gi, 'ê³ ë§ˆì›Œ');
    cleaned = cleaned.replace(/ë¯¸ì•ˆí•´ìš”|ì£„ì†¡í•©ë‹ˆë‹¤/gi, 'ë¯¸ì•ˆí•´');
    cleaned = cleaned.replace(/í•©ë‹ˆ(ë‹¤|ê¹Œ)/gi, 'í•´');
    cleaned = cleaned.replace(/í•˜ê² (ìŠµë‹ˆë‹¤|ì–´ìš”)?/gi, 'í• ê²Œ');

    // 4. ì˜ˆì§„ì´/ë¬´ì¿  1ì¸ì¹­ ì²˜ë¦¬ ë°˜ë³µ(ëˆ„ë½ ë°©ì§€)
    cleaned = cleaned.replace(/ë¬´ì¿ ê°€/g, 'ë‚´ê°€')
        .replace(/ë¬´ì¿ ëŠ”/g, 'ë‚˜ëŠ”')
        .replace(/ë¬´ì¿ ë¥¼/g, 'ë‚˜ë¥¼')
        .replace(/ì˜ˆì§„ì´ê°€/g, 'ë‚´ê°€')
        .replace(/ì˜ˆì§„ì´ëŠ”/g, 'ë‚˜ëŠ”')
        .replace(/ì˜ˆì§„ì´ë¥¼/g, 'ë‚˜ë¥¼');

    // 5. ë¶ˆí•„ìš”í•œ ë¬¸ì, ì—°ì† ê³µë°± ì •ë¦¬
    cleaned = cleaned.replace(/[\"\'\[\]]/g, '').replace(/\s\s+/g, ' ').trim();

    // 6. ë§Œì•½ "ìê¸°ì•¼"ë‚˜ "ìê¸°"ê°€ í˜¹ì‹œë¼ë„ ë‚¨ì•˜ìœ¼ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ë²ˆ ë” ê°•ì œ ì¹˜í™˜
    cleaned = cleaned.replace(/ìê¸°ì•¼/gi, 'ì•„ì €ì”¨').replace(/ìê¸°/gi, 'ì•„ì €ì”¨');

    // 7. ìµœì†Œ ê¸¸ì´ ë³´ì¥
    if (!cleaned || cleaned.length < 2) {
        return 'ì‘? ë‹¤ì‹œ ë§í•´ë´ ì•„ì €ì”¨';
    }

    return cleaned;
}

// âœ¨ í˜„ì¬ ì„¤ì •ëœ ëª¨ë¸ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ (ë””ë²„ê·¸ìš©)
function getCurrentModelInfo() {
    if (!getCurrentModelSetting) {
        return { setting: 'unknown', model: 'gpt-4o' };
    }
    
    const currentSetting = getCurrentModelSetting();
    let actualModel = 'gpt-4o';
    
    switch(currentSetting) {
        case '3.5':
            actualModel = 'gpt-3.5-turbo';
            break;
        case '4.0':
            actualModel = 'gpt-4o';
            break;
        case 'auto':
            actualModel = 'auto-select';
            break;
    }
    
    return { setting: currentSetting, model: actualModel };
}

// âœ¨ ì•ˆì „í•œ ëª¨ë¸ ê²€ì¦ í•¨ìˆ˜
function validateModel(model) {
    const validModels = ['gpt-3.5-turbo', 'gpt-4o', 'gpt-4-turbo', 'gpt-4'];
    if (!model || !validModels.includes(model)) {
        console.warn(`âš ï¸ [ëª¨ë¸ê²€ì¦] ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸: ${model}, ê¸°ë³¸ê°’ ì‚¬ìš©`);
        return 'gpt-4o';
    }
    return model;
}

module.exports = {
    saveLog,
    saveImageLog,
    callOpenAI,
    cleanReply,
    // âœ¨ ìƒˆë¡œìš´ í•¨ìˆ˜ë“¤ ì¶”ê°€
    determineGptModel,
    getOptimalModelForMessage,
    getModelOptimizedSettings,
    getCurrentModelInfo,
    validateModel
};
